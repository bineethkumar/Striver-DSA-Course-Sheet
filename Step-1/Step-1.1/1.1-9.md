## Time Complexity
Asymptotic notations are mathematical tools to represent the time complexity of algorithms for asymptotic analysis. The following 3 asymptotic notations are mostly used to represent the time complexity of algorithms:
1. Big-O Notation(O):  
We define an algorithm’s worst-case time complexity by using the Big-O notation, which determines the set of functions grows slower than or at the same rate as the expression. Furthermore, it explains the maximum amount of time an algorithm requires to consider all input values.

2. Omega Notation(Ω):  
It defines the best case of an algorithm’s time complexity, the Omega notation defines whether the set of functions will grow faster or at the same rate as the expression. Furthermore, it explains the minimum amount of time an algorithm requires to consider all input values.

3. Theta Notation(Θ):  
It defines the average case of an algorithm’s time complexity, the Theta notation defines when the set of functions lies in both O(expression) and Omega(expression), then Theta notation is used. This is how we define a time complexity average case for an algorithm. 

Shortcut to find Order of Growth:  
$Constant < loglog^n < log^n < n^1/3 < n^1/2 < n < n^2< n^3< n^4 < 2^n < n^n $

### Time Complexity of Loops
1. **O(1)**: The time complexity of a function (or set of statements) is considered as O(1) if it doesn’t contain a loop, recursion, and call to any other non-constant time function i.e. set of non-recursive and non-loop statements.  
```java
for (int i = 1; i <= c; i++) {
    // some O(1) expressions
}
```

2. O(n): The Time Complexity of a loop is considered as O(n) if the loop variables are incremented/decremented by a constant amount.
```java
for (int i = 1; i <= n; i += c) { //O(n)
    // some O(1) expressions
}
   
for (int i = n; i > 0; i -= c) { //O(n)
    // some O(1) expressions
}
```
3. $O(n^c)$: The time complexity is defined as an algorithm whose performance is directly proportional to the squared size of the input data, as in nested loops it is equal to the number of times the innermost statement is executed. 
```java
for (int i = 1; i <= n; i += c) {
    for (int j = 1; j <= n; j += c) {
        // some O(1) expressions
    }
}
 
for (int i = n; i > 0; i -= c) {
    for (int j = i + 1; j <= n; j += c) {
        // some O(1) expressions
    }
}
```
4. O(Log n): The time Complexity of a loop is considered as O(Logn) if the loop variables are divided/multiplied by a constant amount. And also for recursive calls in the recursive function, the Time Complexity is considered as O(Logn).
```java
for (int i = 1; i <= n; i *= c) {
    // some O(1) expressions
}
for (int i = n; i > 0; i /= c) {
    // some O(1) expressions
}

void recurse(n)
{
    if (n == 0)
        return;
    else {
        // some O(1) expressions
    }
    recurse(n - 1);
}
```
5. O(Log Log n): The Time Complexity of a loop is considered as O(LogLogn) if the loop variables are reduced/increased exponentially by a constant amount.
```java
for (int i = 2; i <= n; i = Math.pow(i, c)) {
    // some O(1) expressions
}
// Here fun is sqrt or cuberoot or any other constant root
for (int i = n; i > 1; i = fun(i)) {
    // some O(1) expressions
}
```
